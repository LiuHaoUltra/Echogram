"""è¯­éŸ³æœåŠ¡æ¨¡å— - TTS/ASR åŠŸèƒ½"""

import base64
import aiohttp
from openai import AsyncOpenAI

from core.config_service import config_service
from utils.logger import logger


class VoiceServiceError(Exception):
    """è¯­éŸ³æœåŠ¡åŸºç¡€å¼‚å¸¸"""
    pass


class ASRNotConfiguredError(VoiceServiceError):
    """ASR æ¨¡å‹æœªé…ç½®"""
    pass


class TTSNotConfiguredError(VoiceServiceError):
    """TTS æœªå¯ç”¨æˆ–æœªé…ç½®"""
    pass


class VoiceService:
    """è¯­éŸ³æœåŠ¡ï¼šASRï¼ˆè¯­éŸ³è½¬æ–‡å­—ï¼‰å’Œ TTSï¼ˆæ–‡å­—è½¬è¯­éŸ³ï¼‰"""
    
    async def is_asr_configured(self) -> bool:
        """æ£€æŸ¥ ASR æ˜¯å¦å·²é…ç½®"""
        asr_model = await config_service.get_value("asr_model_name")
        return bool(asr_model)
    
    async def is_tts_configured(self) -> bool:
        """æ£€æŸ¥ TTS æ˜¯å¦å·²é…ç½®"""
        tts_enabled = await config_service.get_value("tts_enabled", "false")
        tts_url = await config_service.get_value("tts_api_url")
        tts_ref_audio = await config_service.get_value("tts_ref_audio_path")
        
        # å®‰å…¨çš„å­—ç¬¦ä¸²æ¯”è¾ƒ
        is_enabled = str(tts_enabled).strip().lower() in ("true", "1", "yes")
        
        if not is_enabled or not tts_url or not tts_ref_audio:
            logger.warning(f"TTS Config Check Failed: Enabled={is_enabled} ({tts_enabled}), URL={bool(tts_url)}, RefAudio={bool(tts_ref_audio)}")
        
        return is_enabled and bool(tts_url) and bool(tts_ref_audio)
    
    async def chat_with_voice(self, voice_file_bytes: bytes, system_prompt: str, history_messages: list) -> str:
        """
        è¯­éŸ³å¤šæ¨¡æ€å¯¹è¯ (Multimodal Audio-to-Text)
        
        Args:
            voice_file_bytes: åŸå§‹è¯­éŸ³æ–‡ä»¶ (OGG)
            system_prompt: å½“å‰äººæ ¼è®¾å®šçš„ System Prompt
            history_messages: å†å²å¯¹è¯ä¸Šä¸‹æ–‡ (OpenAI æ ¼å¼åˆ—è¡¨)
            
        Returns:
            str: åŸå§‹ LLM å“åº”ï¼ŒåŒ…å« <transcript> å’Œ <chat> æ ‡ç­¾
        """
        api_key = await config_service.get_value("api_key")
        base_url = await config_service.get_value("api_base_url")
        asr_model = await config_service.get_value("asr_model_name") # å¤ç”¨ ASR æ¨¡å‹é…ç½®ä½œä¸ºè¯­éŸ³æ¨¡å‹åç§°
        
        if not api_key:
            raise ASRNotConfiguredError("API Key æœªé…ç½®")
        
        # Base64 ç¼–ç éŸ³é¢‘ (OGG -> WAV è½¬æ¢)
        # OpenAI Chat API ä¸æ”¯æŒ OGGï¼Œéœ€è½¬æ¢ä¸º WAV
        import uuid
        import os
        from pydub import AudioSegment
        import io
        
        temp_ogg_path = f"/tmp/{uuid.uuid4()}.ogg"
        temp_wav_path = f"/tmp/{uuid.uuid4()}.wav"
        
        try:
            # 1. ä¿å­˜ OGG åˆ°ä¸´æ—¶æ–‡ä»¶
            with open(temp_ogg_path, "wb") as f:
                f.write(voice_file_bytes)
            
            # 2. è½¬æ¢ä¸º WAV
            audio = AudioSegment.from_ogg(temp_ogg_path)
            audio.export(temp_wav_path, format="wav")
            
            # 3. è¯»å– WAV å¹¶ç¼–ç 
            with open(temp_wav_path, "rb") as f:
                wav_bytes = f.read()
                base64_audio = base64.b64encode(wav_bytes).decode('utf-8')
                
        except Exception as e:
            logger.error(f"éŸ³é¢‘æ ¼å¼è½¬æ¢å¤±è´¥: {e}")
            raise VoiceServiceError(f"éŸ³é¢‘é¢„å¤„ç†å¤±è´¥: {e}")
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_ogg_path):
                os.remove(temp_ogg_path)
            if os.path.exists(temp_wav_path):
                os.remove(temp_wav_path)
        
        # --- æ„é€ å¤šæ¨¡æ€ Messages ---
        
        # 1. System Prompt (æ³¨å…¥è¯­éŸ³æ¨¡å¼åè®®)
        voice_protocol = (
            "\n\n# VOICE MODE PROTOCOL [CRITICAL]\n"
            "You are currently processing a direct Voice Message from the user.\n"
            "Your output MUST strictly follow this XML structure:\n\n"
            "<transcript>...Transcribe the user's speech verbatim here...</transcript>\n"
            "<chat>...Your natural, conversational reply here (following all Soul/Protocol rules)...</chat>\n\n"
            "Example:\n"
            "<transcript>Hello, what time is it?</transcript>\n"
            "<chat>It's 10 PM. <chat react=\"ğŸ˜´\">Time for bed?</chat></chat>"
        )
        
        final_system_prompt = system_prompt + voice_protocol
        
        # 2. æ„å»ºä¸Šä¸‹æ–‡
        messages = []
        messages.append({"role": "system", "content": final_system_prompt})
        
        # æ’å…¥å†å²è®°å½• (ä»…æœ€è¿‘å‡ æ¡ï¼Œé¿å… Token è¿‡é•¿)
        if history_messages:
            messages.extend(history_messages[-10:])
            
        # 3. å½“å‰è¯­éŸ³æ¶ˆæ¯
        user_content = [
            {
                "type": "text",
                "text": "Please process this audio message according to the Voice Mode Protocol."
            },
            {
                "type": "input_audio",
                "input_audio": {
                    "data": base64_audio,
                    "format": "wav"
                }
            }
        ]
        messages.append({"role": "user", "content": user_content})

        try:
            logger.info(f"VoiceChat: è°ƒç”¨æ¨¡å‹ {asr_model} (Multimodal)...")
            
            client = AsyncOpenAI(api_key=api_key, base_url=base_url)
            # 400 Bad Request Fix: gpt-audio-mini ä¾ç„¶éœ€è¦ modalities=["text"] å—ï¼Ÿ
            # å®˜æ–¹æ–‡æ¡£æ˜¾ç¤º Audio Output æš‚æœªå®Œå…¨å¼€æ”¾ API (å³ modalities=["audio", "text"])ï¼Œ
            # è¿™é‡Œæˆ‘ä»¬åªè¯·æ±‚æ–‡å­—å›å¤ï¼Œæ‰€ä»¥ä¿æŒ modalities=["text"] æ˜¯å®‰å…¨çš„ï¼Œç”šè‡³å¯èƒ½æ˜¯å¿…é¡»çš„ã€‚
            response = await client.chat.completions.create(
                model=asr_model,
                messages=messages,
                temperature=0.7, # ç¨å¾®å…è®¸ä¸€ç‚¹åˆ›é€ æ€§ï¼Œåæ­£æœ‰ transcript çº¦æŸ
                modalities=["text"]
            )
            
            if not response.choices or not response.choices[0].message.content:
                logger.warning(f"VoiceChat è¿”å›ç©ºå†…å®¹")
                return ""
            
            raw_output = response.choices[0].message.content.strip()
            logger.info(f"VoiceChat æˆåŠŸ: {raw_output[:100]}...")
            
            return raw_output
            
        except Exception as e:
            logger.error(f"VoiceChat è°ƒç”¨å¤±è´¥: {e}")
            raise VoiceServiceError(f"è¯­éŸ³æœåŠ¡ä¸å¯ç”¨: {e}")
    
    async def text_to_speech(self, text: str) -> bytes:
        """
        TTSï¼šæ–‡å­—è½¬è¯­éŸ³ï¼ˆä½¿ç”¨ GPT-SoVITS APIï¼‰
        
        Args:
            text: å¾…åˆæˆçš„æ–‡å­—
            
        Returns:
            è¯­éŸ³æ–‡ä»¶å­—èŠ‚æµï¼ˆOGG æ ¼å¼ï¼‰
            
        Raises:
            TTSNotConfiguredError: TTS æœªé…ç½®
            VoiceServiceError: è¯­éŸ³åˆæˆå¤±è´¥
        """
        # è¾“å…¥éªŒè¯
        if not text or not text.strip():
            raise VoiceServiceError("å¾…åˆæˆæ–‡æœ¬ä¸ºç©º")
        
        # æ£€æŸ¥é…ç½®
        tts_enabled = await config_service.get_value("tts_enabled", "false")
        is_enabled = str(tts_enabled).strip().lower() in ("true", "1", "yes")
        if not is_enabled:
            raise TTSNotConfiguredError("TTS åŠŸèƒ½æœªå¯ç”¨")
        
        tts_url = await config_service.get_value("tts_api_url")
        tts_ref_audio = await config_service.get_value("tts_ref_audio_path")
        
        if not tts_url or not tts_ref_audio:
            raise TTSNotConfiguredError("TTS API URL æˆ–å‚è€ƒéŸ³é¢‘è·¯å¾„æœªé…ç½®")
        
        # å®‰å…¨è½¬æ¢ speed_factor
        try:
            speed_factor_str = await config_service.get_value("tts_speed_factor", "1.0")
            speed_factor = float(speed_factor_str)
            # é™åˆ¶èŒƒå›´ 0.5-2.0
            speed_factor = max(0.5, min(2.0, speed_factor))
        except (ValueError, TypeError):
            logger.warning(f"æ— æ•ˆçš„ speed_factor é…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼ 1.0")
            speed_factor = 1.0
        
        # å‡†å¤‡è¯·æ±‚å‚æ•°
        payload = {
            "text": text.strip(),
            "text_lang": await config_service.get_value("tts_text_lang", "zh"),
            "ref_audio_path": tts_ref_audio,
            "prompt_lang": await config_service.get_value("tts_prompt_lang", "zh"),
            "prompt_text": "",
            "media_type": "ogg",  # Telegram å…¼å®¹æ ¼å¼
            "speed_factor": speed_factor
        }
        
        try:
            logger.info(f"TTS: åˆæˆè¯­éŸ³ ({len(text)} å­—ç¬¦)...")
            
            # ç›´æ¥ä½¿ç”¨é…ç½®çš„ URL (éµå¾ªç”¨æˆ·è¾“å…¥)
            api_endpoint = tts_url

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    api_endpoint,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        raise VoiceServiceError(f"TTS API è¿”å›é”™è¯¯: {response.status} - {error_text}")
                    
                    audio_bytes = await response.read()
                    logger.info(f"TTS æˆåŠŸ: ç”Ÿæˆ {len(audio_bytes)} å­—èŠ‚è¯­éŸ³æ–‡ä»¶")
                    
                    return audio_bytes
                    
        except TTSNotConfiguredError:
            # é‡æ–°æŠ›å‡ºé…ç½®é”™è¯¯
            raise
        except aiohttp.ClientError as e:
            logger.error(f"TTS ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
            raise VoiceServiceError(f"TTS ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
        except VoiceServiceError:
            # é‡æ–°æŠ›å‡ºå·²çŸ¥çš„è¯­éŸ³æœåŠ¡é”™è¯¯
            raise
        except Exception as e:
            logger.error(f"TTS åˆæˆå¤±è´¥ï¼ˆæœªé¢„æœŸé”™è¯¯ï¼‰: {e}")
            raise VoiceServiceError(f"TTS åˆæˆå¤±è´¥: {e}")
    
    async def get_last_user_message_type(self, chat_id: int) -> str:
        """
        è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯çš„ç±»å‹
        
        Args:
            chat_id: èŠå¤© ID
            
        Returns:
            æ¶ˆæ¯ç±»å‹: 'text' æˆ– 'voice'
        """
        from sqlalchemy import select
        from config.database import get_db_session
        from models.history import History
        
        async for session in get_db_session():
            stmt = select(History.message_type) \
                .where(History.chat_id == chat_id, History.role == "user") \
                .order_by(History.timestamp.desc()) \
                .limit(1)
            
            result = await session.execute(stmt)
            message_type = result.scalar_one_or_none()
            
            return message_type or "text"


voice_service = VoiceService()
