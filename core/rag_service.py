import re
import asyncio
import json
import time
from typing import List, Dict, Any, Optional
from sqlalchemy import select, text, and_, bindparam
from openai import AsyncOpenAI
from config.settings import settings
from config.database import get_db_session
from core.config_service import config_service
from models.history import History
from models.rag_status import RagStatus
from utils.logger import logger
import html

class RagService:
    # é»˜è®¤é…ç½®å¸¸é‡
    DEFAULT_SIMILARITY_THRESHOLD = 0.6
    DEFAULT_TOP_K = 5
    SYNC_COOLDOWN_SECONDS = 60 # æ¯ 2 åˆ†é’Ÿè§¦å‘ä¸»å¾ªç¯ï¼Œæ¯ 1 åˆ†é’Ÿå…è®¸å•ä¸ª Chat é‡çˆ¬

    def __init__(self):
        self._client = None
        self._current_api_key = None
        self._current_base_url = None
        self._sync_cooldowns: Dict[int, float] = {}  # chat_id -> last_failure_time

    def _etl_debug(self, msg: str):
        """RAG ETL è°ƒè¯•æ—¥å¿—ï¼ˆé»˜è®¤å…³é—­ï¼Œé¿å…æ—¥å¿—è†¨èƒ€ï¼‰ã€‚"""
        if settings.RAG_VERBOSE_LOG:
            logger.info(msg)

    async def _notify_admin(self, text: str):
        """å‘é€ç§ä¿¡ç»™ç®¡ç†å‘˜ (å†…éƒ¨è°ƒè¯•/é€æ˜åŒ–ä½¿ç”¨)"""
        if not settings.RAG_NOTIFY_ADMIN:
            return
        from core.bot import bot
        if bot and settings.ADMIN_USER_ID:
            try:
                # å°½é‡ä¿æŒé™é»˜ï¼Œå¦‚æœæŠ¥é”™ä¹Ÿä¸é˜»å¡ä¸»æµç¨‹
                await bot.send_message(
                    chat_id=settings.ADMIN_USER_ID,
                    text=text,
                    parse_mode="HTML",
                    disable_web_page_preview=True
                )
            except Exception as e:
                logger.error(f"ETL Notify Admin failed: {e}")
    
    async def _get_client(self):
        """è·å–æˆ–åˆå§‹åŒ– OpenAI Client (æ”¯æŒåŠ¨æ€é…ç½®æ›´æ–°)"""
        configs = await config_service.get_all_settings()
        api_key = configs.get("api_key")
        base_url = configs.get("api_base_url")
        
        if not api_key:
             raise ValueError("API Key not configured")

        # æ£€æŸ¥é…ç½®æ˜¯å¦å˜æ›´
        if (not self._client or 
            api_key != self._current_api_key or 
            base_url != self._current_base_url):
            
            self._client = AsyncOpenAI(api_key=api_key, base_url=base_url)
            self._current_api_key = api_key
            self._current_base_url = base_url
            # logger.info("RAG Client (re)initialized with new config.")
            
        return self._client

    async def _get_summary_model(self):
        """è·å–é…ç½®çš„æ‘˜è¦/æ¸…æ´—æ¨¡å‹"""
        configs = await config_service.get_all_settings()
        # ä¼˜å…ˆä½¿ç”¨ summary_model_name, é™çº§ä½¿ç”¨ model_name
        model = configs.get("summary_model_name") or configs.get("model_name")
        return model

    async def denoise_interaction(self, user_content: str, ai_content: str) -> str:
        """
        [ETL Phase 1] ä½¿ç”¨ LLM è¿›è¡Œè¯­ä¹‰é™å™ª
        å°† User+AI çš„å®Œæ•´å¯¹è¯è½®æ¬¡è½¬åŒ–ä¸ºé«˜å¯†åº¦çš„å®¢è§‚äº‹å®ã€‚
        """
        model_name = await self._get_summary_model()
        if not model_name:
            return f"User asked: {user_content}\nAI answered: {ai_content}"

        sys_prompt = (
            "ä½ æ˜¯ä¸€å RAG çŸ¥è¯†åº“æ„å»ºä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·çš„æé—®å’Œ AI çš„å›å¤æ¸…æ´—ä¸ºä¸€æ¡â€œé«˜å¯†åº¦â€çš„äº‹å®è®°å½•ã€‚\n"
            "è§„åˆ™ï¼š\n"
            "1. **æå–æ ¸å¿ƒ**ï¼šæå–ç”¨æˆ·é‡åˆ°çš„å…·ä½“é—®é¢˜ï¼ˆæŠ¥é”™ä¿¡æ¯ã€ä»£ç ä¸Šä¸‹æ–‡ï¼‰å’Œ AI ç»™å‡ºçš„å…³é”®å»ºè®®ã€‚\n"
            "2. **å»é™¤å™ªéŸ³**ï¼šå½»åº•åˆ é™¤æ‰€æœ‰å¯’æš„ï¼ˆâ€œä½ å¥½â€ã€â€œè°¢è°¢â€ï¼‰ã€æƒ…ç»ªè¯ï¼ˆâ€œçƒ¦æ­»äº†â€ï¼‰ã€å£è¯­åºŸè¯ï¼ˆâ€œé‚£ä¸ª...â€ï¼‰ã€‚\n"
            "3. **æŒ‡ä»£æ¶ˆæ­§**ï¼šå¦‚æœç”¨æˆ·è¯´â€œå®ƒæŒ‚äº†â€ï¼Œè¯·æ ¹æ®ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰æˆ–ç›´æ¥ä¿ç•™åŸè¯ä½†å°è¯•è¡¥å……èƒŒæ™¯ã€‚\n"
            "4. **æ ¼å¼**ï¼šè¾“å‡ºä¸ºç¬¬ä¸‰äººç§°é™ˆè¿°å¥ã€‚ä¾‹å¦‚ï¼šâ€œç”¨æˆ·è¯¢é—® Docker å¯åŠ¨å¤±è´¥ (Exit 137)ã€‚AI è§£é‡Šä¸º OOM å¹¶å»ºè®®å¢åŠ  Swapã€‚â€\n"
            "5. **åªè¾“å‡ºç»“æœ**ï¼Œä¸è¦åŒ…å«ä»»ä½•å‰ç¼€æˆ–è§£é‡Šã€‚"
        )

        user_prompt = f"User Input:\n{user_content}\n\nAI Response:\n{ai_content}"

        try:
            client = await self._get_client()
            resp = await client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=300,
                temperature=0.3
            )
            if resp.choices and resp.choices[0].message.content:
                return resp.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"Denoise failed: {e}")
        
        # Fallback
        return f"User: {user_content}\nAI: {ai_content}"

    def sanitize_content(self, text: str) -> str:
        """
        æ¸…æ´—å†…å®¹ï¼š
        1. ä¼˜å…ˆæå– <chat> æ ‡ç­¾å†…çš„å†…å®¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚
        2. å¦‚æœæ²¡æœ‰ <chat> æ ‡ç­¾ï¼ˆå¦‚ç”¨æˆ·æ¶ˆæ¯ï¼‰ï¼Œåˆ™é€€å›å¸¸è§„æ¸…æ´—ã€‚
        """
        if not text:
            return ""
            
        # 0. é¢„å¤„ç†
        
        # ç‰¹æ®Šå¤„ç† Image Summaryï¼Œä¿ç•™è¯­ä¹‰
        # [Image Summary: cute cat] -> å›¾ç‰‡å†…å®¹: cute cat
        text = re.sub(r'\[Image Summary\s*:(.*?)\]', r'å›¾ç‰‡å†…å®¹:\1', text, flags=re.IGNORECASE)

        # å»é™¤ç³»ç»Ÿå ä½ç¬¦ (é˜²æ­¢å™ªéŸ³è¿›å…¥å‘é‡åº“)
        placeholders = [
            "[Voice: Processing...]",
            "[Image: Processing...]"
        ]
        for ph in placeholders:
            text = text.replace(ph, "") 

        # 1. å°è¯•æå– <chat> æ ‡ç­¾å†…å®¹
        # æ³¨æ„: å†å²è®°å½•ä¸­çš„ chat æ ‡ç­¾å¯èƒ½åŒ…å«å±æ€§ (å¦‚ reply="123"), éœ€å…¼å®¹ <chat...>
        # å¯¹åº” SenderService ç”Ÿæˆæ ¼å¼: <chat reply="...">...</chat>
        chat_matches = re.findall(r'<chat[^>]*>(.*?)</chat>', text, flags=re.DOTALL | re.IGNORECASE)
        
        if chat_matches:
            # å¦‚æœå­˜åœ¨ <chat> æ ‡ç­¾ï¼Œåªä¿ç•™æ ‡ç­¾å†…çš„å†…å®¹
            # æ‹¼æ¥å¤šæ®µ chat å†…å®¹
            full_content = " ".join([m.strip() for m in chat_matches])
            return re.sub(r'\s+', ' ', full_content).strip()
            
        # 2. Fallback: å¦‚æœæ²¡æœ‰ <chat> æ ‡ç­¾ (å¸¸è§äº User æ¶ˆæ¯æˆ–æ—§æ•°æ®)
        # ä»ç„¶å»é™¤å¯èƒ½å­˜åœ¨çš„å…¶ä»– XML æ ‡ç­¾ä»¥é˜²å™ªéŸ³ï¼Œä½†ä¿ç•™æ–‡æœ¬
        text = re.sub(r'<[^>]+>', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

    async def _embed_texts(self, texts: List[str]) -> List[List[float]]:
        """è°ƒç”¨ OpenAI API è·å– Embeddings"""
        if not texts:
            return []
            
        try:
            client = await self._get_client()
            configs = await config_service.get_all_settings()
            model_name = configs.get("vector_model_name", "text-embedding-3-small")

            resp = await client.embeddings.create(
                input=texts,
                model=model_name
            )
            
            # å¼ºè¡Œæˆªæ–­åˆ° 1536 ç»´
            # æ— è®ºæ¨¡å‹åŸç”Ÿç»´åº¦æ˜¯å¤šå°‘ï¼Œç»Ÿä¸€æˆªæ–­ä»¥é€‚é… sqlite-vec è¡¨ç»“æ„ã€‚
            # å¯¹äºæ”¯æŒ Matryoshka çš„æ¨¡å‹ï¼ˆå¦‚ OpenAI v3, Geminiï¼‰ï¼Œå‰ 1536 ç»´å³ä¸ºæœ‰æ•ˆè¡¨å¾ã€‚
            return [data.embedding[:1536] for data in resp.data]
        except Exception as e:
            logger.error(f"Embedding API failed: {e}")
            raise

    async def run_background_sync(self):
        """
        [ETL Core] åå°åŒæ­¥å¾ªç¯ (The "Cron")
        ç­–ç•¥: Context Barrier + Turn-based Assembly + Denoising
        åªå¤„ç†å·²ç»è·Œå‡ºæ´»è·ƒçª—å£ (Tier 1 -> Tier 2) çš„æ¶ˆæ¯ã€‚
        """
        self._etl_debug("RAG ETL: Starting background sync cycle...")
        
        # 1. è·å–æ‰€æœ‰æ´»è·ƒçš„ Chat ID
        # ç®€å•èµ·è§ï¼Œä» Recent History æ‰¾ï¼Œæˆ–è€…éå†æ‰€æœ‰ Chat é…ç½®ã€‚
        # è¿™é‡Œå…ˆåªæ‰«ææœ€è¿‘æ´»è·ƒçš„ Top 20 Chat
        async for session in get_db_session():
            try:
                # Find chats with recent activity (Exclude Private Chats: chat_id < 0)
                recent_chats_res = await session.execute(
                    text("SELECT DISTINCT chat_id FROM history WHERE chat_id < 0 ORDER BY id DESC LIMIT 20")
                )
                chat_ids = [r.chat_id for r in recent_chats_res.fetchall()]
                
                for chat_id in chat_ids:
                    await self._process_chat_etl(session, chat_id)
                    
            except Exception as e:
                logger.error(f"RAG ETL Global Loop failed: {e}")

    async def _process_chat_etl(self, session, chat_id: int):
        """å¤„ç†å•ä¸ª Chat çš„ ETL"""
        from core.history_service import history_service
        
        try:
            # 1. è®¡ç®— Context Barrier (æ´»è·ƒçª—å£è¾¹ç•Œ)
            configs = await config_service.get_all_settings()
            max_tokens = int(configs.get("history_tokens", 4000)) # Default 4k
            
            # Reuse the EXACT same logic as /stats to determine the "Active Window" boundary
            # This prevents any discrepancy between what User sees and what RAG sees.
            stats = await history_service.get_session_stats(chat_id, max_tokens)
            active_window_start_id = stats["win_start_id"]
            
            self._etl_debug(f"RAG ETL: Chat {chat_id} | BarrierID (from HistoryService): {active_window_start_id}")
            
            if active_window_start_id == 0:
                 return




            # 2. æ‰«æ T2 åŒº (ID < active_window_start_id) ä¸­çš„æœªå¤„ç†é¡¹
            # æ¡ä»¶: æ˜¯ Assistant æ¶ˆæ¯ (Turn End), ä¸” rag_status ä¸ºç©º
            # ä¸”ä¸åŒ…å« 'Processing...'
            stmt_candidates = text("""
                SELECT h.id 
                FROM history h
                LEFT JOIN rag_status s ON h.id = s.msg_id
                WHERE h.chat_id = :cid
                  AND h.id < :barrier
                  AND h.role = 'assistant'
                  AND s.msg_id IS NULL
                  AND h.content NOT LIKE '[%: Processing...]'
                ORDER BY h.id ASC
                LIMIT 50
            """)
            
            cand_res = await session.execute(stmt_candidates, {"cid": chat_id, "barrier": active_window_start_id})
            candidate_ids = [r.id for r in cand_res.fetchall()]
            
            if not candidate_ids:
                # --- è¯Šæ–­ä¸è‡ªåŠ¨æ¸…ç†é€»è¾‘ ---
                # 1. æŸ¥æ‰¾ Barrier ä»¥ä¸‹çš„æ‰€æœ‰æœªå¤„ç†æ¶ˆæ¯
                stmt_pending_all = text("""
                    SELECT h.id, h.role, SUBSTR(h.content, 1, 50) as snippet 
                    FROM history h
                    LEFT JOIN rag_status s ON h.id = s.msg_id
                    WHERE h.chat_id = :cid AND h.id < :barrier AND s.msg_id IS NULL
                    LIMIT 50
                """)
                res = await session.execute(stmt_pending_all, {"cid": chat_id, "barrier": active_window_start_id})
                all_orphans = res.fetchall()
                
                if all_orphans:
                    processed_ids = []
                    # 2. è‡ªåŠ¨æ¸…ç†: System æ¶ˆæ¯ç›´æ¥æ ‡è®° SKIPPED, è·ç¦» Barrier å¤ªè¿œ(>30)çš„ç”¨æˆ·æ¶ˆæ¯æ ‡è®° SKIPPED
                    for o in all_orphans:
                        if o.role == 'system' or (active_window_start_id - o.id > 30):
                            await session.execute(
                                text("INSERT OR IGNORE INTO rag_status (msg_id, chat_id, status, processed_at) VALUES (:id, :cid, 'SKIPPED', CURRENT_TIMESTAMP)"),
                                {"id": o.id, "cid": chat_id}
                            )
                            processed_ids.append(o.id)
                    
                    if processed_ids:
                        await session.commit()
                        self._etl_debug(f"RAG ETL: Auto-cleaned {len(processed_ids)} orphans (System/Old) for Chat {chat_id}.")
                        await self._notify_admin(f"ğŸ§¹ <b>ETL è‡ªåŠ¨æ¸…ç† [Chat {chat_id}]</b>\nå·²æ¸…ç† {len(processed_ids)} æ¡ç³»ç»Ÿ/è¿‡æ—¶æ¶ˆæ¯ï¼ˆè¿™äº›æ¶ˆæ¯é€šå¸¸ä¸å« RAG ä»·å€¼ï¼‰ã€‚")
                return

            self._etl_debug(f"RAG ETL: Chat {chat_id} has {len(candidate_ids)} candidates falling out of context (barrier: {active_window_start_id}).")

            # 3. Process each Candidate (Turn Assembly)
            for anchor_id in candidate_ids:
                await self._process_single_turn(session, chat_id, anchor_id)
                
        except Exception as e:
            logger.error(f"RAG ETL failed for chat {chat_id}: {e}")

    async def _process_single_turn(self, session, chat_id: int, anchor_id: int):
        """
        å¤„ç†å•ä¸ªäº¤äº’è½®æ¬¡
        anchor_id æ˜¯ AI çš„ä¸€æ¡æ¶ˆæ¯ IDã€‚éœ€å‘å‰/å‘åæ‹¼è£…å®Œæ•´è½®æ¬¡ã€‚
        """
        # 3.1 Gather AI Block (Backwards & Forwards)
        # æˆ‘ä»¬çš„ Anchor æ˜¯ Candidate æ‰«å‡ºæ¥çš„ï¼Œå¯èƒ½æ˜¯ AI Block çš„ä¸­é—´æŸä¸€æ¡ã€‚
        # ä½†æˆ‘ä»¬ä¹‹å‰é€»è¾‘æ˜¯ï¼šCandidate æ˜¯ "Unprocessed Assistant Msg".
        # åªè¦æˆ‘ä»¬å¤„ç†å®Œæ ‡è®°äº†ï¼Œå°±ä¸ä¼šé‡å¤æ‰«ã€‚
        
        # è¿™é‡Œçš„ç­–ç•¥ï¼šä»¥ Anchor ä¸ºæ ¸å¿ƒï¼Œå‘åæ‰¾ AI (ç›´åˆ° User), å‘å‰æ‰¾ AI (ç›´åˆ° User) ç»„æˆ AI Block.
        # ç„¶åå†å‘å‰æ‰¾ User ç»„æˆ User Block.
        
        # ç®€åŒ–ç­–ç•¥: 
        # 1. Anchor å¿…å®šæ˜¯ AIã€‚
        # 2. å‘å‰æ‰¾è¿ç»­ AI -> åˆå¹¶
        # 3. å†å‘å‰æ‰¾è¿ç»­ User -> åˆå¹¶ä¸º User Block
        
        # Look back for AI chain start
        # å…¶å®æ›´ç®€å•çš„åšæ³•æ˜¯ï¼šæ¯æ¬¡åªå¤„ç† AI Block çš„**æœ€åä¸€æ¡**ä½œä¸º Headï¼Ÿ
        # ä¸è¡Œï¼Œå› ä¸ºæˆ‘ä»¬æ‰«å‡ºçš„æ˜¯ "æ‰€æœ‰æœªå¤„ç†çš„ AI"ã€‚
        # å¦‚æœä¸€ä¸ª AI Block æœ‰ 3 æ¡ï¼Œæˆ‘ä»¬ä¼šæ‰«å‡º 3 ä¸ª Candidateã€‚
        # æˆ‘ä»¬å¤„ç†ç¬¬ä¸€ä¸ªæ—¶ï¼Œå¦‚æœä¸æŠŠåé¢ä¸¤ä¸ªæ ‡è®°æ‰ï¼Œä¸‹æ¬¡å¾ªç¯è¿˜ä¼šæ‰«åˆ°ã€‚
        
        # æ‰€ä»¥ï¼šä¸€æ—¦å¤„ç†ï¼Œå¿…é¡»æŠŠæ•´ä¸ª Block çš„ ID éƒ½æ ‡è®°å¥½ã€‚
        
        # Fetch surrounding messages (Window 20 is enough for a turn)
        stmt_surround = text("""
            SELECT id, role, content FROM history 
            WHERE chat_id=:cid AND id BETWEEN :low AND :high
            ORDER BY id ASC
        """)
        rows = (await session.execute(stmt_surround, {"cid": chat_id, "low": anchor_id - 10, "high": anchor_id + 5})).fetchall()
        
        # Find Anchor index
        anchor_idx = -1
        for i, r in enumerate(rows):
            if r.id == anchor_id:
                anchor_idx = i
                break
        
        if anchor_idx == -1: return # Should not happen

        # Expand AI Block
        ai_ids = [anchor_id]
        ai_content = [rows[anchor_idx].content]
        
        # Look forward (Next is AI?)
        curr = anchor_idx + 1
        while curr < len(rows) and rows[curr].role == 'assistant':
            ai_ids.append(rows[curr].id)
            ai_content.append(rows[curr].content)
            curr += 1
            
        # Look backward (Prev is AI?)
        curr = anchor_idx - 1
        while curr >= 0 and rows[curr].role == 'assistant':
            ai_ids.insert(0, rows[curr].id)
            ai_content.insert(0, rows[curr].content)
            curr -= 1
            
        # Update Anchor to be the LAST ID of the AI Block (Standard Convention)
        real_head_id = ai_ids[-1]
        
        # å¦‚æœ real_head_id å·²ç»è¢«å¤„ç†è¿‡(åœ¨ rag_status é‡Œ)ï¼Œé‚£æ•´ä¸ª Block éƒ½è·³è¿‡
        # (check DB)
        chk = await session.execute(text("SELECT 1 FROM rag_status WHERE msg_id=:mid"), {"mid": real_head_id})
        if chk.scalar():
            # Mark curent anchor as SKIPPED just in case
             if anchor_id != real_head_id:
                 await session.execute(text("INSERT OR IGNORE INTO rag_status (msg_id, chat_id, status, processed_at) VALUES (:id, :cid, 'SKIPPED', CURRENT_TIMESTAMP)"), 
                                       {"id": anchor_id, "cid": chat_id})
                 await session.commit()
             return

        # Look backward for User Block (User Question)
        # Start searching from before the first AI msg
        user_ids = []
        user_content = []
        
        search_idx = -1
        # Find index of first AI msg in 'rows'
        first_ai_id = ai_ids[0]
        for i, r in enumerate(rows):
            if r.id == first_ai_id:
                search_idx = i - 1
                break
        
        while search_idx >= 0:
            if rows[search_idx].role == 'user':
                user_ids.insert(0, rows[search_idx].id) # Prepend
                user_content.insert(0, rows[search_idx].content)
                search_idx -= 1
            elif rows[search_idx].role == 'system':
                # é‡åˆ°ç³»ç»Ÿæ¶ˆæ¯ (ä¾‹å¦‚ Reaction)ï¼Œæ ‡è®°ä¸º TAIL/SKIPPED å¹¶ç»§ç»­å‘å‰å›æº¯
                # è¿™è§£å†³äº†ç³»ç»Ÿæ¶ˆæ¯æ‰“æ–­ç”¨æˆ·æ¶ˆæ¯é“¾çš„é—®é¢˜
                await session.execute(
                    text("INSERT OR IGNORE INTO rag_status (msg_id, chat_id, status, processed_at) VALUES (:id, :cid, 'SKIPPED', CURRENT_TIMESTAMP)"),
                    {"id": rows[search_idx].id, "cid": chat_id}
                )
                search_idx -= 1
            else:
                # é‡åˆ°å…¶ä»–è§’è‰² (é€šå¸¸æ˜¯ä¸Šä¸€è½®çš„ Assistant)ï¼Œåœæ­¢å›æº¯
                break
            
        # Assembly
        full_user_text = "\n".join(user_content)
        full_ai_text = "\n".join(ai_content)
        
        if not full_user_text:
            # Orphan AI response? Maybe system msg before?
            full_user_text = "(Context missing or System trigger)"
            
        # 4. Denoise
        denoised_text = await self.denoise_interaction(full_user_text, full_ai_text)
        
        # 5. Embed
        vecs = await self._embed_texts([denoised_text])
        if not vecs: return
        vector = vecs[0]
        
        # 6. Store
        # Head (Last AI ID) -> HEAD + Vector + Denoised
        # Others -> TAIL/SKIPPED
        
        # Head
        await session.execute(
            text("""
                INSERT OR REPLACE INTO rag_status (msg_id, chat_id, status, denoised_content, processed_at) 
                VALUES (:id, :cid, 'HEAD', :content, CURRENT_TIMESTAMP)
            """), 
            {"id": real_head_id, "cid": chat_id, "content": denoised_text}
        )
        
        await session.execute(
            text("INSERT INTO history_vec(rowid, embedding) VALUES (:id, :vec)"),
            {"id": real_head_id, "vec": json.dumps(vector)}
        )
        
        # Tails (Other AI parts)
        for aid in ai_ids:
            if aid != real_head_id:
                await session.execute(
                    text("INSERT OR IGNORE INTO rag_status (msg_id, chat_id, status, processed_at) VALUES (:id, :cid, 'TAIL', CURRENT_TIMESTAMP)"),
                    {"id": aid, "cid": chat_id}
                )
                
        # Users (Linked parts)
        for uid in user_ids:
            await session.execute(
                text("INSERT OR IGNORE INTO rag_status (msg_id, chat_id, status, processed_at) VALUES (:id, :cid, 'TAIL', CURRENT_TIMESTAMP)"),
                {"id": uid, "cid": chat_id}
            )

        await session.commit()
        self._etl_debug(f"RAG ETL: Indexed Turn {real_head_id} (User: {len(user_ids)}, AI: {len(ai_ids)})")
        
        # 7. é€šçŸ¥ç®¡ç†å‘˜
        msg = (
            f"âœ… <b>RAG ETL å®Œæˆ</b>\n"
            f"ğŸ“ Chat: <code>{chat_id}</code>\n"
            f"ğŸ”— Turn Head: {real_head_id}\n\n"
            f"<b>ğŸ§  äº‹å®åŒ–å†…å®¹ (Denoised):</b>\n"
            f"<code>{html.escape(denoised_text)}</code>"
        )
        await self._notify_admin(msg)

    async def contextualize_query(self, query_text: str, conversation_history: str, long_term_summary: str = "") -> str:
        """
        [Query Rewriting]
        ä½¿ç”¨æ‘˜è¦æ¨¡å‹å¿«é€Ÿé‡å†™æŸ¥è¯¢ï¼Œæ¶ˆé™¤æŒ‡ä»£ä¸æ˜ã€‚
        ç°åœ¨æ¥æ”¶ä¸ä¸»æ¨¡å‹å®Œå…¨ä¸€è‡´çš„ Full Context (Active Window + Summary)ã€‚
        """
        # [DEBUG] Log entry
        self._etl_debug(f"RAG Rewriter: Input='{query_text}' (Len: {len(query_text)})")



        try:
            configs = await config_service.get_all_settings()
            summary_model = configs.get("summary_model_name")
            
            # å¦‚æœæ²¡é…æ‘˜è¦æ¨¡å‹ï¼Œåˆ™é™çº§ä½¿ç”¨ä¸»æ¨¡å‹ï¼›å¦‚æœä¸»æ¨¡å‹ä¹Ÿæ²¡é…ï¼Œåˆ™è·³è¿‡
            if not summary_model:
                summary_model = configs.get("model_name")
            
            if not summary_model:
                logger.warning("RAG Rewriter: Skipped (No Model Configured)")
                return query_text

            self._etl_debug(f"RAG Rewriter: Using model '{summary_model}'")

            client = await self._get_client()
            
            # æ„å»ºè½»é‡çº§ Context
            # context_msgs åº”è¯¥æ˜¯ ["User: ...", "Assistant: ..."] çš„æœ€è¿‘å‡ æ¡
            sys_prompt = (
                "ä½ æ˜¯ä¸€åæŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ã€‚"
                "ä½ çš„ç›®æ ‡æ˜¯å°†ç”¨æˆ·çš„æœ€æ–°è¾“å…¥é‡å†™ä¸ºé€‚åˆæ•°æ®åº“æ£€ç´¢çš„ç®€æ´æŸ¥è¯¢è¯­å¥ã€‚"
                "1. æŒ‡ä»£æ¶ˆæ­§ï¼šå°†'å®ƒ'ã€'é‚£ä¸ª'ç­‰ä»£è¯æ›¿æ¢ä¸ºä¸Šä¸‹æ–‡ä¸­è®¨è®ºçš„å…·ä½“å¯¹è±¡ã€‚"
                "2. è¡¥å……èƒŒæ™¯ï¼šå¦‚æœç”¨æˆ·çš„è¯ä¾èµ–å‰æ–‡ï¼ˆå¦‚è¿½é—®åŸå› ï¼‰ï¼Œè¯·æŠŠä¸»è¯­å’ŒèƒŒæ™¯è¡¥å…¨ã€‚"
                "3. å¤šæ¨¡æ€èåˆï¼šå¦‚æœè¾“å…¥åŒ…å« [Image Summary: ...]ï¼Œä¸”å¯¹æŸ¥è¯¢æœ‰å¸®åŠ©ï¼Œè¯·æå–æ ¸å¿ƒè¯­ä¹‰ã€‚å¯¹äºè¯­éŸ³è½¬å½•çš„æ–‡æœ¬ï¼Œç›´æ¥è§†ä¸ºç”¨æˆ·å¯¹ç™½çš„æœ‰æ•ˆç»„æˆéƒ¨åˆ†ã€‚"
                "4. å»å™ªç²¾ç®€ï¼šåšå†³å»é™¤æ‰€æœ‰æƒ…ç»ªè¯ï¼ˆå¦‚'å“æ­»'ã€'å“ˆå“ˆ'ï¼‰ã€å£è¯­åºŸè¯ï¼ˆå¦‚'æˆ‘æƒ³æƒ³'ã€'ä¸çŸ¥é“'ï¼‰å’ŒæŠ±æ€¨ã€‚åªä¿ç•™äº‹å®æ€§å…³é”®è¯ã€‚"
                "5. è¾“å‡ºæ ¼å¼ï¼šè¾“å‡ºä¸€å¥æ¸…æ™°ã€å®¢è§‚çš„é™ˆè¿°å¥æˆ–é—®å¥ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šã€‚"
                "åªè¾“å‡ºé‡å†™åçš„å­—ç¬¦ä¸²ã€‚"
            )
            
            # Construct Rich Context Block
            full_context_block = ""
            if long_term_summary:
                full_context_block += f"=== Long-term Memory ===\n{long_term_summary}\n\n"
            
            full_context_block += f"=== Active Conversation ===\n{conversation_history}"

            resp = await client.chat.completions.create(
                model=summary_model,
                messages=[
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": f"{full_context_block}\n\nUser Input to Rewrite:\n{query_text}"}
                ],
                max_tokens=200,
                temperature=0.3
            )
            
            if resp.choices and resp.choices[0].message.content:
                new_query = resp.choices[0].message.content.strip()
                # ç§»é™¤å¯èƒ½è¯¯å¸¦çš„å¼•å·
                if new_query.startswith('"') and new_query.endswith('"'):
                    new_query = new_query[1:-1]
                
                if new_query != query_text:
                    self._etl_debug(f"RAG Rewriter: '{query_text}' -> '{new_query}'")
                    try:
                        import core.bot as bot_module
                        if settings.RAG_NOTIFY_ADMIN and bot_module.bot:
                            rewrite_msg = (
                                f"ğŸ”„ <b>RAG Query Rewritten</b>\n"
                                f"From: <code>{html.escape(query_text)}</code>\n"
                                f"To: <code>{html.escape(new_query)}</code>"
                            )
                            await bot_module.bot.send_message(settings.ADMIN_USER_ID, rewrite_msg, parse_mode='HTML')
                    except Exception as notify_e:
                        logger.error(f"Failed to send rewrite debug: {notify_e}")
                else:
                    self._etl_debug(f"RAG Rewriter: No change ('{new_query}').")
                
                return new_query
            
        except Exception as e:
            logger.warning(f"Query Rewrite failed: {e}")
            
        return query_text

    async def search_context(self, chat_id: int, query_text: str, exclude_ids: Optional[List[int]] = None, top_k: int = 5, context_padding: int = 3) -> str:
        """
        æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ (Context Window Expansion)
        
        ç­–ç•¥:
        1. Vector Search: æ‰¾åˆ° Top K æ ¸å¿ƒåŒ¹é… (Anchors).
        2. Expansion: å¯¹æ¯ä¸ª Anchorï¼ŒåŸºäºé€»è¾‘é¡ºåºæŸ¥è¯¢å‰åæ–‡ (è§£å†³ ID Gap é—®é¢˜).
        3. Clustering: åˆå¹¶é‡å çš„ä¸Šä¸‹æ–‡çª—å£.
        4. Formatting: è¾“å‡ºå¸¦ç„¦ç‚¹çš„å¯¹è¯å—.

        :param exclude_ids: éœ€è¦æ’é™¤çš„æ¶ˆæ¯ ID åˆ—è¡¨ (é¿å…è‡ªå¼•ç”¨)
        :param context_padding: æ¯ä¸ª Anchor å‰åæ‰©å±•çš„æ¶ˆæ¯æ•°é‡
        """
        sanitized_query = self.sanitize_content(query_text)
        if len(sanitized_query) < 2:
            return ""

        limit = top_k if top_k else self.DEFAULT_TOP_K
        
        configs = await config_service.get_all_settings()
        
        # åŠ¨æ€è¯»å– Padding é…ç½®
        current_padding = context_padding
        try:
            if val := configs.get("rag_context_padding"):
                current_padding = int(val)
        except: pass
        
        threshold = self.DEFAULT_SIMILARITY_THRESHOLD
        try:
            if val := configs.get("rag_similarity_threshold"):
                threshold = float(val)
        except: pass

        # [DEBUG] Start Notification
        try:
            import core.bot as bot_module
            if bot_module.bot:
                start_msg = (
                    f"ğŸ” <b>RAG Search: Interaction Mode</b>\n"
                    f"Chat: <code>{chat_id}</code> | Q: <code>{html.escape(sanitized_query)}</code>\n"
                    f"TopK: {limit} | Pad: {current_padding}"
                )
                await bot_module.bot.send_message(settings.ADMIN_USER_ID, start_msg, parse_mode='HTML')
        except: pass

        try:
            # 1. Get Query Vector
            query_vecs = await self._embed_texts([sanitized_query])
            if not query_vecs:
                print(f"[DEBUG] No query vector generated", file=sys.stderr)
                return ""
            query_vec = query_vecs[0]
            
            async for session in get_db_session():
                # ---------------------------------------------------------
                # Step 1: Vector Search (Find Anchors)
                # ---------------------------------------------------------
                exclusion_clause = ""
                params = {
                    "chat_id": chat_id, 
                    "query_vec": json.dumps(query_vec),
                    "threshold": threshold,
                    "top_k": limit
                }
                
                if exclude_ids:
                    exclusion_clause = "AND h.id NOT IN :exclude_ids"
                    params["exclude_ids"] = tuple(exclude_ids)

                anchor_sql = f"""
                    SELECT h.id, vec_distance_cosine(v.embedding, :query_vec) as distance
                    FROM history_vec v
                    JOIN history h ON v.rowid = h.id
                    WHERE h.chat_id = :chat_id 
                      AND vec_distance_cosine(v.embedding, :query_vec) < :threshold
                      {exclusion_clause}
                    ORDER BY distance ASC
                    LIMIT :top_k
                """
                
                stmt = text(anchor_sql)
                if exclude_ids:
                    stmt = stmt.bindparams(bindparam("exclude_ids", expanding=True))
                
                result = await session.execute(stmt, params)
                anchors = result.fetchall()  # [(id, distance), ...]
                
                if not anchors:
                    return ""
                
                anchor_map = {row.id: row.distance for row in anchors}
                sorted_anchor_ids = [row.id for row in anchors]

                # ---------------------------------------------------------
                # Step 2: Logical Expansion (Fixing ID Gaps)
                # ---------------------------------------------------------
                # Clusters: List[Set[int]] - åˆå§‹æ¯ä¸ª Anchor ä¸€ä¸ª Cluster
                clusters: List[set] = []

                for anchor_id in sorted_anchor_ids:
                    # è·å–å‰æ–‡ (Pre-context)
                    # å€’åºå– limitï¼Œç»“æœéœ€åè½¬
                    pre_sql = text("""
                        SELECT id FROM history 
                        WHERE chat_id = :cid AND id < :aid 
                        ORDER BY id DESC LIMIT :pad
                    """)
                    pre_res = await session.execute(pre_sql, {"cid": chat_id, "aid": anchor_id, "pad": current_padding})
                    pre_ids = [r.id for r in pre_res.fetchall()]
                    
                    # è·å–åæ–‡ (Post-context)
                    post_sql = text("""
                        SELECT id FROM history 
                        WHERE chat_id = :cid AND id > :aid 
                        ORDER BY id ASC LIMIT :pad
                    """)
                    post_res = await session.execute(post_sql, {"cid": chat_id, "aid": anchor_id, "pad": current_padding})
                    post_ids = [r.id for r in post_res.fetchall()]




                    # ç»„è£…å½“å‰ Cluster
                    current_cluster = set(pre_ids + [anchor_id] + post_ids)
                    clusters.append(current_cluster)

                # ---------------------------------------------------------
                # Step 3: Cluster Merging
                # ---------------------------------------------------------
                # è´ªå©ªåˆå¹¶ï¼šå¦‚æœæœ‰äº¤é›†ï¼Œåˆ™åˆå¹¶
                merged_clusters: List[set] = []
                
                while clusters:
                    base = clusters.pop(0)
                    # å°è¯•ä¸åç»­æ‰€æœ‰ cluster åˆå¹¶
                    i = 0
                    while i < len(clusters):
                        candidate = clusters[i]
                        if not base.isdisjoint(candidate):
                            base.update(candidate)
                            clusters.pop(i) # ç§»é™¤å·²è¢«åˆå¹¶çš„
                        else:
                            i += 1
                    merged_clusters.append(base)

                # ---------------------------------------------------------
                # Step 4: Content Fetching
                # ---------------------------------------------------------
                # æ”¶é›†æ‰€æœ‰éœ€è¦æŸ¥è¯¢çš„ Unique ID
                all_needed_ids = set()
                for c in merged_clusters:
                    all_needed_ids.update(c)
                
                if not all_needed_ids:
                    return ""

                # æ‰¹é‡è·å–å†…å®¹
                fetch_sql = text("SELECT id, role, content, timestamp FROM history WHERE id IN :ids")
                fetch_stmt = fetch_sql.bindparams(bindparam("ids", expanding=True))
                fetch_res = await session.execute(fetch_stmt, {"ids": tuple(all_needed_ids)})
                
                # ID -> Message Object
                msg_map = {
                    row.id: {
                        "role": row.role,
                        "content": row.content,
                        "timestamp": row.timestamp
                    } 
                    for row in fetch_res.fetchall()
                }

                # ---------------------------------------------------------
                # Step 5: Formatting with Focus Highlighting
                # ---------------------------------------------------------
                output_blocks = []
                
                # å¯¹ Merged Clusters æŒ‰å…¶ä¸­æœ€å° ID æ’åºï¼Œä¿è¯æ—¶é—´åº
                merged_clusters.sort(key=lambda s: min(s))

                for cluster in merged_clusters:
                    # Cluster å†…éƒ¨æŒ‰ ID æ’åº
                    sorted_ids = sorted(list(cluster))
                    block_lines = []
                    
                    for mid in sorted_ids:
                        msg = msg_map.get(mid)
                        if not msg: continue
                        
                        # Date Formatting
                        date_str = "Unknown"
                        if msg["timestamp"]:
                            if hasattr(msg["timestamp"], 'strftime'):
                                date_str = msg["timestamp"].strftime("%Y-%m-%d %H:%M")
                            else:
                                date_str = str(msg["timestamp"])[:16]

                        content = self.sanitize_content(msg["content"])
                        line = f"[{date_str}] {msg['role'].capitalize()}: {content}"

                        # Check if this is an Anchor
                        if mid in anchor_map:
                            dist = anchor_map[mid]
                            # Highlight Anchor
                            line = f">>> {line} (Match: {dist:.3f}) <<<"
                        
                        block_lines.append(line)
                    
                    output_blocks.append("\n".join(block_lines))

                # Join blocks with explicit separator
                final_context = "\n\n... (Context Skip) ...\n\n".join(output_blocks)

                # [DEBUG] Success Notification
                try:
                    import core.bot as bot_module
                    if bot_module.bot:
                        debug_msg = (
                            f"âœ… <b>RAG Result: Interaction Mode</b>\n"
                            f"Blocks: {len(output_blocks)} | Total Msgs: {len(all_needed_ids)}\n"
                            f"<pre>{html.escape(final_context[:3000])}</pre>" # Truncate for TG
                        )
                        await bot_module.bot.send_message(settings.ADMIN_USER_ID, debug_msg, parse_mode='HTML')
                except: pass

                return final_context

        except Exception as e:
            logger.error(f"RAG Search failed: {e}", exc_info=True)
            return ""

    async def clear_chat_vectors(self, chat_id: int):
        """
        æ¸…é™¤æŒ‡å®šä¼šè¯çš„æ‰€æœ‰å‘é‡æ•°æ®å’ŒRAGçŠ¶æ€ (ç‰©ç†åˆ é™¤)
        ç”¨äº /reset æˆ– Rebuild Index
        """
        async for session in get_db_session():
            try:
                # 1. Clear rag_status (The Knowledge Base)
                await session.execute(
                    text("DELETE FROM rag_status WHERE chat_id = :chat_id"),
                    {"chat_id": chat_id}
                )

                # 2. Clear history_vec (The Vector Index)
                # é€šè¿‡å­æŸ¥è¯¢åˆ é™¤ history_vec ä¸­å¯¹åº”çš„ rowid
                # å‡è®¾ history_vec æ˜¯è™šæ‹Ÿè¡¨æˆ–æ™®é€šè¡¨ï¼Œrowid å¯¹åº” history.id
                await session.execute(
                    text("""
                        DELETE FROM history_vec 
                        WHERE rowid IN (
                            SELECT id FROM history WHERE chat_id = :chat_id
                        )
                    """),
                    {"chat_id": chat_id}
                )
                await session.commit()
                
                # æ¸…é™¤å†·å´çŠ¶æ€ï¼Œå…è®¸ç«‹å³é‡æ–°åŒæ­¥
                if chat_id in self._sync_cooldowns:
                    del self._sync_cooldowns[chat_id]
                    
                logger.info(f"RAG: Cleared all vectors & status for chat {chat_id}")
            except Exception as e:
                logger.error(f"RAG Clear failed for chat {chat_id}: {e}")

    async def clear_all_vectors(self):
        """
        æ¸…é™¤æ‰€æœ‰ä¼šè¯çš„å‘é‡æ•°æ® (å…¨å±€é‡ç½®)
        """
        async for session in get_db_session():
            try:
                # 1. Clear rag_status
                await session.execute(text("DELETE FROM rag_status"))

                # 2. Clear history_vec
                await session.execute(text("DELETE FROM history_vec"))
                
                await session.commit()
                self._sync_cooldowns.clear()
                logger.info("RAG: Global Index Cleared.")
            except Exception as e:
                logger.error(f"Global RAG Clear failed: {e}")

    async def rebuild_index(self, chat_id: int = None):
        """
        Rebuild Index
        å¦‚æœæŒ‡å®š chat_idï¼Œåªæ¸…é™¤è¯¥ä¼šè¯ã€‚
        å¦‚æœä¸æŒ‡å®š (None)ï¼Œåˆ™æ¸…é™¤æ‰€æœ‰ (Global).
        """
        if chat_id:
            await self.clear_chat_vectors(chat_id)
        else:
            await self.clear_all_vectors()

    async def get_vector_stats(self, chat_id: int) -> Dict[str, Any]:
        """
        è·å–æŒ‡å®šä¼šè¯çš„ RAG ç»Ÿè®¡ (v2)
        åˆ†ç±»:
        1. Knowledge Base (Indexed): å·²é™å™ªå¹¶å…¥åº“çš„ Facts.
        2. Pending (ETL Queue): å·²è·Œå‡ºä¸Šä¸‹æ–‡çª—å£ï¼Œç­‰å¾…é™å™ªçš„.
        3. Active (Hot): ä»åœ¨ä¸Šä¸‹æ–‡çª—å£å†…ï¼Œæ— éœ€ç´¢å¼•çš„.
        """
        async for session in get_db_session():
            try:
                # 1. è®¡ç®— Context Barrier (Call HistoryService directly)
                from core.history_service import history_service
                
                configs = await config_service.get_all_settings()
                max_tokens = int(configs.get("history_tokens", 4000))

                # Reuse exact logic
                stats = await history_service.get_session_stats(chat_id, max_tokens)
                barrier_id = stats["win_start_id"]
                
                # Simple count for active window
                stmt_active_count = text("SELECT COUNT(*) FROM history WHERE chat_id=:cid AND id >= :barrier")
                active_count = 0
                if barrier_id > 0:
                    active_count = (await session.execute(stmt_active_count, {"cid": chat_id, "barrier": barrier_id})).scalar() or 0



                # 2. Count Indexed (HEAD)
                stmt_indexed = text("SELECT COUNT(*) FROM rag_status WHERE chat_id = :cid AND status = 'HEAD'")
                indexed_count = (await session.execute(stmt_indexed, {"cid": chat_id})).scalar() or 0

                # 3. Count Pending (Assistant Msgs < Barrier NOT IN rag_status)
                pending_count = 0
                if barrier_id > 0:
                    stmt_pending = text("""
                        SELECT COUNT(*) FROM history h
                        LEFT JOIN rag_status s ON h.id = s.msg_id
                        WHERE h.chat_id = :cid
                          AND h.id < :barrier
                          AND s.msg_id IS NULL
                          AND h.content NOT LIKE '[%: Processing...]'
                    """)
                    pending_count = (await session.execute(stmt_pending, {"cid": chat_id, "barrier": barrier_id})).scalar() or 0

                # 4. Count Active (Approximate)
                # Just return a status string or boolean?
                # Let's count Assistant msgs in active window for completeness

                # Re-query simplified for active assistant count if needed, or just omit.
                # User cares about: "How many indexed?" vs "How many waiting?"

                # Cooldown
                cooldown_left = 0
                if chat_id in self._sync_cooldowns:
                     passed = time.time() - self._sync_cooldowns[chat_id]
                     if passed < self.SYNC_COOLDOWN_SECONDS:
                         cooldown_left = int(self.SYNC_COOLDOWN_SECONDS - passed)

                return {
                    "indexed": indexed_count,
                    "pending": pending_count,
                    "active_window_size": active_count, # Msgs in active window
                    "cooldown_left": cooldown_left
                }
            except Exception as e:
                logger.error(f"RAG Stats failed: {e}")
                return {"error": str(e)}

rag_service = RagService()
