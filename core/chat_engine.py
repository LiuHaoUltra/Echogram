from telegram import Update, constants
from telegram.ext import ContextTypes
from openai import AsyncOpenAI
import re
import asyncio
import pytz

from core.access_service import access_service
from core.history_service import history_service
from core.config_service import config_service
from core.summary_service import summary_service
from config.settings import settings
from config.database import get_db_session
from models.history import History
from sqlalchemy import select, delete
from core.secure import is_admin
from core.lazy_sender import lazy_sender
from core.media_service import media_service, TTSNotConfiguredError, MediaServiceError
from utils.logger import logger
from utils.prompts import prompt_builder
from utils.config_validator import safe_int_config, safe_float_config
from core.sender_service import sender_service
from core.rag_service import rag_service
from collections import defaultdict

# ä¼šè¯çº§ RAG é”ï¼Œé˜²æ­¢å¹¶å‘å¯¼è‡´é‡å¤åµŒå…¥
CHAT_LOCKS = defaultdict(asyncio.Lock)


async def process_message_entry(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    HTTP/Telegram æ¶ˆæ¯å…¥å£ (æ–‡æœ¬)
    1. é‰´æƒ
    2. å­˜å…¥å†å²
    3. æ”¾å…¥ç¼“å†²é˜Ÿåˆ— (LazySender)
    """
    user = update.effective_user
    chat = update.effective_chat
    message = update.message
    
    # ç©ºå€¼æ£€æŸ¥ï¼šuserã€chatã€message å¿…é¡»å­˜åœ¨
    if not user or not chat or not message or not message.text:
        return
        
    # æŒ‡ä»¤äº¤ç”± CommandHandler å¤„ç†
    if message.text.strip().startswith('/'):
        return

    # --- 1. è®¿é—®æ§åˆ¶ ---
    is_adm = is_admin(user.id)
    
    if chat.type == constants.ChatType.PRIVATE:
        # ç§èŠï¼šä»…ç®¡ç†å‘˜å¯è§ï¼Œä½†ä¸ä½œä¸ºèŠå¤©è®°å½•å¤„ç†
        if is_adm:
            pass
        return
    else:
        # ç¾¤ç»„ï¼šå¿…é¡»åœ¨ç™½åå•å†…
        if not await access_service.is_whitelisted(chat.id):
            return
            
    # é€šè¿‡é‰´æƒåè®°å½•æ—¥å¿—
    logger.info(f"MSG [{chat.id}] from {user.first_name}: {message.text[:20]}...")

    # å­˜å…¥å†å²
    reply_to_id = None
    reply_to_content = None
    
    if message.reply_to_message:
        reply_to_id = message.reply_to_message.message_id
        raw_ref_text = message.reply_to_message.text or "[Non-text message]"
        reply_to_content = (raw_ref_text[:30] + "..") if len(raw_ref_text) > 30 else raw_ref_text

    await history_service.add_message(
        chat.id, 
        "user", 
        message.text, 
        message_id=message.message_id,
        reply_to_id=reply_to_id,
        reply_to_content=reply_to_content
    )
    
    # è§¦å‘èšåˆ (ä¼ é€’ dedup_id ä»¥æ”¯æŒ Edits å¹¶é˜²é‡å¤)
    await lazy_sender.on_message(chat.id, context, dedup_id=update.update_id)

    try:
        asyncio.create_task(summary_service.check_and_summarize(chat.id))
    except Exception as e:
        logger.error(f"Failed to trigger proactive summary: {e}")


async def process_photo_entry(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    å›¾ç‰‡æ¶ˆæ¯å…¥å£ (èšåˆæ¨¡å¼)
    """
    user = update.effective_user
    chat = update.effective_chat
    message = update.message
    
    if not user or not chat or not message or not message.photo:
        return
        
    # --- 1. è®¿é—®æ§åˆ¶ ---
    is_adm = is_admin(user.id)
    if chat.type == constants.ChatType.PRIVATE:
        if is_adm: pass
        return
    else:
        if not await access_service.is_whitelisted(chat.id):
            return
            
    logger.info(f"PHOTO [{chat.id}] from {user.first_name}")
    
    # è·å–æœ€å¤§å°ºå¯¸å›¾ç‰‡
    photo = message.photo[-1]
    file_id = photo.file_id
    
    # å­˜å…¥å†å² (å ä½)
    reply_to_id = None
    reply_to_content = None
    if message.reply_to_message:
        reply_to_id = message.reply_to_message.message_id
        raw_text = message.reply_to_message.text or "[Non-text message]"
        reply_to_content = (raw_text[:30] + "..") if len(raw_text) > 30 else raw_text

    # è·å– Caption 
    caption = message.caption or ""
    db_content = f"[Image: Processing...]{caption}"

    await history_service.add_message(
        chat.id, "user", db_content, 
        message_id=message.message_id,
        reply_to_id=reply_to_id, reply_to_content=reply_to_content,
        message_type="image", file_id=file_id
    )
    
    # è§¦å‘èšåˆ (ä¼ é€’ dedup_id ä»¥æ”¯æŒ Edits å¹¶é˜²é‡å¤)
    await lazy_sender.on_message(chat.id, context, dedup_id=update.update_id)
    try:
        asyncio.create_task(summary_service.check_and_summarize(chat.id))
    except:
        pass


async def process_voice_message_entry(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    è¯­éŸ³æ¶ˆæ¯å…¥å£ (èšåˆæ¨¡å¼)
    """
    user = update.effective_user
    chat = update.effective_chat
    message = update.message
    
    # ç©ºå€¼æ£€æŸ¥
    if not user or not chat or not message or not message.voice:
        return
    
    # --- 1. è®¿é—®æ§åˆ¶ ---
    is_adm = is_admin(user.id)
    if chat.type == constants.ChatType.PRIVATE:
        if is_adm: pass
        return
    else:
        if not await access_service.is_whitelisted(chat.id):
            return
    
    logger.info(f"VOICE [{chat.id}] from {user.first_name}: {message.voice.duration}s")
    
    file_id = message.voice.file_id
    
    # å­˜å…¥å†å² (å ä½)
    reply_to_id = None
    reply_to_content = None
    if message.reply_to_message:
        reply_to_id = message.reply_to_message.message_id
        raw_text = message.reply_to_message.text or "[Non-text message]"
        reply_to_content = (raw_text[:30] + "..") if len(raw_text) > 30 else raw_text

    await history_service.add_message(
        chat.id, "user", "[Voice: Processing...]",
        message_id=message.message_id,
        reply_to_id=reply_to_id, reply_to_content=reply_to_content,
        message_type="voice", file_id=file_id
    )
    
    # è§¦å‘èšåˆ (ä¼ é€’ dedup_id ä»¥æ”¯æŒ Edits å¹¶é˜²é‡å¤)
    await lazy_sender.on_message(chat.id, context, dedup_id=update.update_id)
    try:
        asyncio.create_task(summary_service.check_and_summarize(chat.id))
    except:
        pass


async def generate_response(chat_id: int, context: ContextTypes.DEFAULT_TYPE):
    """
    æ ¸å¿ƒå›å¤ç”Ÿæˆé€»è¾‘ (æ”¯æŒå¤šæ¨¡æ€èšåˆ)
    1. è·å–å†å²
    2. æ‰«æ Recent Assistant ä¹‹åçš„ User Messages
    3. æå– pending çš„å›¾ç‰‡/è¯­éŸ³å¹¶ä¸‹è½½è½¬æ¢
    4. æ„é€  Multimodal Payload
    5. è°ƒç”¨ LLM
    6. è§£æç»“æœ (Summary/Transcript) å¹¶å›å¡« DB
    7. å‘é€å›å¤
    """
    logger.info(f"Generate Response triggered for Chat {chat_id}")
    
    configs = await config_service.get_all_settings()
    api_key = configs.get("api_key")
    base_url = configs.get("api_base_url")
    model = configs.get("model_name", "gpt-3.5-turbo")
    system_prompt_custom = configs.get("system_prompt")
    timezone = configs.get("timezone", "UTC")

    if not api_key:
        await context.bot.send_message(chat_id, "âš ï¸ å°šæœªé…ç½® API Keyï¼Œè¯·ä½¿ç”¨ /dashboard é…ç½®ã€‚")
        return

    dynamic_summary = await summary_service.get_summary(chat_id)

        # --- RAG Integration & Core Locking ---
        # æŒ‰ç…§æŒ‡ç¤ºï¼Œæ•´ä¸ªç”Ÿæˆè¿‡ç¨‹éœ€è¦åœ¨é”å†…æ‰§è¡Œï¼Œä»¥ä¿è¯ Strict Serialization
    async with CHAT_LOCKS[chat_id]:
        rag_context = ""
        # [RAG Sync Removed from Hot Path]
        # sync_historic_embeddings is now deprecated and moved to background ETL task.
        
        # Token limit check
        target_tokens = safe_int_config(
            configs.get("history_tokens"),
            settings.HISTORY_WINDOW_TOKENS,
            min_val=100, max_val=50000
        )
        
        # 1. è·å–åŸºç¡€å†å²è®°å½•
        history_msgs = await history_service.get_token_controlled_context(chat_id, target_tokens=target_tokens)
        
        # 2. è¯†åˆ«â€œå°¾éƒ¨â€èšåˆåŒºé—´ 
        last_assistant_idx = -1
        for i in range(len(history_msgs) - 1, -1, -1):
            if history_msgs[i].role == 'assistant':
                last_assistant_idx = i
                break
                
        if last_assistant_idx == -1:
            tail_msgs = history_msgs
            base_msgs = []
        else:
            base_msgs = history_msgs[:last_assistant_idx+1]
            tail_msgs = history_msgs[last_assistant_idx+1:]
    
        # --- Shift-Left: Multimodal Pre-processing ---
        # åœ¨ RAG æœç´¢ä¹‹å‰ï¼Œå…ˆå¤„ç† Pending çš„å›¾ç‰‡å’Œè¯­éŸ³ï¼Œè·å– Caption/Transcript
        # è¿™æ · RAG Rewrite å°±èƒ½åˆ©ç”¨è¿™äº›ä¿¡æ¯
        # ç¼“å­˜å¤„ç†ç»“æœï¼Œé¿å…åç»­é‡å¤ä¸‹è½½
        processed_media_cache = {} # msg_id -> (type, content_text)
        
        pending_images_map = {}
        pending_voices_map = {} # Initialize this map as it's used in process_media_item
        # --- Shift-Left: Multimodal Pre-processing (Parallelized) ---
        # å¹¶è¡Œå¤„ç†æ‰€æœ‰å¾…å¤„ç†çš„å›¾ç‰‡å’Œè¯­éŸ³ï¼Œä»¥æœ€å¤§åŒ– TTFT
        tasks = []
        
        async def process_media_item(msg):
            # 1. Image Processing
            if msg.message_type == 'image' and msg.file_id and "[Image: Processing...]" in msg.content:
                try:
                    f = await context.bot.get_file(msg.file_id)
                    b = await f.download_as_bytearray()
                    file_bytes = bytes(b)
                    
                    # Call Media Model (Captioning)
                    # Use generic XML Protocol
                    caption = await media_service.caption_image(file_bytes)
                    
                    # Cache & Update Content using Legacy Format
                    # Format: [Image Summary: caption]
                    processed_media_cache[msg.message_id] = ("image", caption)
                    msg.content = f"[Image Summary: {caption}]"
                    
                    # Store for later rendering
                    pending_images_map[msg.message_id] = (msg, file_bytes)
                except Exception as e:
                    logger.error(f"Shift-Left Image failed: {e}")
                    msg.content = "[Image Summary: Analyze Failed]"

            # 2. Voice Processing
            elif msg.message_type == 'voice' and msg.file_id and "[Voice: Processing...]" in msg.content:
                try:
                    f = await context.bot.get_file(msg.file_id)
                    b = await f.download_as_bytearray()
                    file_bytes = bytes(b)
                    
                    # Call Media Model (Transcription)
                    # Use generic XML Protocol
                    transcript = await media_service.transcribe_audio(file_bytes)
                    
                    # Cache & Update Content using Legacy Format
                    # Format: Raw Text
                    processed_media_cache[msg.message_id] = ("voice", transcript)
                    msg.content = transcript
                    
                    # Store
                    pending_voices_map[msg.message_id] = (msg, file_bytes)
                except Exception as e:
                    logger.error(f"Shift-Left Voice failed: {e}")
                    msg.content = "[Voice Transcript Failed]"

        # Create tasks for all tail messages
        if tail_msgs:
            for msg in tail_msgs:
                if msg.message_type in ('image', 'voice'):
                    tasks.append(process_media_item(msg))
            
            if tasks:
                logger.info(f"Shift-Left: Processing {len(tasks)} media items in parallel...")
                await asyncio.gather(*tasks)


        # --- RAG Search ---
        # ç§»åˆ°é”å†…æ‰§è¡Œï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°çš„ embeddings
        try:
            # èšåˆå½“å‰è½®æ¬¡ä¸­æ‰€æœ‰çš„ç”¨æˆ·æ–‡æœ¬æ¶ˆæ¯ä½œä¸ºæŸ¥è¯¢è¯ (æ­¤æ—¶å·²åŒ…å«å¤šæ¨¡æ€è½¬æ¢åçš„æ–‡æœ¬)
            user_texts = [
                m.content for m in tail_msgs 
                if m.role == 'user' and m.content
            ]
            current_query = " ".join(user_texts).strip()
            
            if current_query:
                # æ”¶é›†å½“å‰ä¸Šä¸‹æ–‡ä¸­çš„æ‰€æœ‰æ¶ˆæ¯ ID ä»¥æ’é™¤ (Self-Echo Prevention)
                # åŒ…æ‹¬ base_msgs å’Œ tail_msgs
                context_ids = [m.id for m in history_msgs if m.id]
                
                # --- Query Rewriting (Contextualization) ---
                # å‡†å¤‡å®Œæ•´ä¸Šä¸‹æ–‡ç»™ Rewriter (ä¸ä¸»æ¨¡å‹å¯¹é½)
                # åŒ…å«: 1. Long-term Summary; 2. All History in Active Window
                
                full_history_lines = []
                for m in history_msgs:
                    # ç®€å•æ ¼å¼åŒ– content, ä¸æˆªæ–­ (Trust the model/token limit of rewriter)
                    full_history_lines.append(f"{m.role.capitalize()}: {m.content}")
                
                full_history_str = "\n".join(full_history_lines)
                
                rewritten_query = await rag_service.contextualize_query(
                    query_text=current_query, 
                    conversation_history=full_history_str,
                    long_term_summary=dynamic_summary
                )
                
                found_context = await rag_service.search_context(
                    chat_id, 
                    rewritten_query, 
                    exclude_ids=context_ids
                )
                
                if found_context:
                    rag_context = found_context
                    logger.info(f"RAG: Injected memory for '{current_query[:20]}...'")
        except Exception as e:
            logger.error(f"RAG Search Error: {e}")
    
        if rag_context:
            dynamic_summary += f"\n\n[Relevant Long-term Memories]\n{rag_context}"

        # 3. å‡†å¤‡ç³»ç»Ÿæç¤ºè¯
        # åªè¦æœ«å°¾å­˜åœ¨è¯­éŸ³æˆ–å›¾ç‰‡ï¼Œå°±å¯ç”¨å¯¹åº”çš„å¤šæ¨¡æ€åè®®
        has_v = any(m.message_type == 'voice' for m in tail_msgs)
        has_i = any(m.message_type == 'image' for m in tail_msgs)
    


    # 4. æ£€æŸ¥ä¸Šä¸€è½®è¡¨æƒ…è¿è§„æƒ…å†µ (Reaction Violation Check)
    has_rv = False
    if last_assistant_idx != -1:
        last_assistant_msg = history_msgs[last_assistant_idx]
        # è§£ææ ‡ç­¾ä¸­çš„ react å±æ€§
        react_matches = re.finditer(r'react=["\']([^"\']+)["\']', last_assistant_msg.content)
        for rm in react_matches:
            full_react = rm.group(1).strip()
            emoji_part = full_react.split(":")[0].strip() if ":" in full_react else full_react
            if emoji_part not in sender_service.TG_FREE_REACTIONS:
                has_rv = True
                break
    
    system_content = prompt_builder.build_system_prompt(
        soul_prompt=system_prompt_custom, 
        timezone=timezone, 
        dynamic_summary=dynamic_summary,
        has_voice=has_v,
        has_image=has_i,
        reaction_violation=has_rv
    )
    
    messages = [{"role": "system", "content": system_content}]
    
    # æ—¶åŒºå¤„ç†
    import pytz
    try:
        tz = pytz.timezone(timezone)
    except:
        tz = pytz.UTC

    # 4. å¡«å……åŸºç¡€å†å² (base_msgs)
    for h in base_msgs:
        time_str = "Unknown"
        if h.timestamp:
            try:
                dt = h.timestamp.replace(tzinfo=pytz.UTC) if h.timestamp.tzinfo is None else h.timestamp
                time_str = dt.astimezone(tz).strftime("%Y-%m-%d %H:%M:%S")
            except: pass
        
        msg_id_str = f"MSG {h.message_id}" if h.message_id else "MSG ?"
        msg_type_str = h.message_type.capitalize() if h.message_type else "Text"
        prefix = f"[{msg_id_str}] [{time_str}] [{msg_type_str}] "
        if h.reply_to_content:
            prefix += f'(Reply to "{h.reply_to_content}") '
        messages.append({"role": h.role, "content": prefix + h.content})

    # 5. æ‰«æèšåˆåŒºé—´å†…çš„ Pending å†…å®¹ (Using Pre-processed Cache)
    # pending_images_map = {msg_id: (msg_obj, file_bytes)}
    # pending_voices_map = {msg_id: (msg_obj, file_bytes)}
    
    has_multimodal = bool(pending_images_map or pending_voices_map)
    
    # è¿˜éœ€è¦æ£€æŸ¥æ˜¯å¦æœ‰çº¯æ–‡æœ¬çš„ tail messages éœ€è¦åŠ å…¥
    # å¦‚æœ tail_msgs é‡Œæœ‰ id ä¸åœ¨ pending map é‡Œï¼Œä¸”æ˜¯ user æ–‡æœ¬ï¼Œä¹Ÿç®— multimodal batch å—ï¼Ÿ
    # ç»Ÿä¸€é€»è¾‘ï¼šåªè¦æœ‰ tail_msgsï¼Œå°±é‡ç»„ä¸º user message list
    
    if tail_msgs:
        multimodal_content = []
        
        for msg in tail_msgs:
            # Time & Prefix
            time_str = "Unknown"
            if msg.timestamp:
                try:
                    dt = msg.timestamp.replace(tzinfo=pytz.UTC) if msg.timestamp.tzinfo is None else msg.timestamp
                    time_str = dt.astimezone(tz).strftime("%Y-%m-%d %H:%M:%S")
                except: pass
            
            msg_id_str = f"MSG {msg.message_id}" if msg.message_id else "MSG ?"
            msg_type_str = msg.message_type.capitalize() if msg.message_type else "Text"
            prefix = f"[{msg_id_str}] [{time_str}] [{msg_type_str}] "

            # Image
            if msg.message_id in pending_images_map:
                msg_obj, file_bytes = pending_images_map[msg.message_id]
                # è·å– XML (Shift-Left å·²æ›´æ–° msg.content)
                # content: <img_summary ...>...</img_summary>
                
                try:
                    b64 = await media_service.process_image_to_base64(file_bytes)
                    if b64:
                        multimodal_content.append({"type": "text", "text": f"{prefix}{msg.content}"})
                        multimodal_content.append({"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64}"}})
                except Exception as e:
                    logger.error(f"Image B64 failed: {e}")
                    multimodal_content.append({"type": "text", "text": f"{prefix}[Image Error]"})
            
            # Voice
            elif msg.message_id in pending_voices_map:
                msg_obj, file_bytes = pending_voices_map[msg.message_id]
                # content: <transcript ...>...</transcript>
                
                try:
                    b64 = await media_service.process_audio_to_base64(file_bytes)
                    if b64:
                        multimodal_content.append({"type": "text", "text": f"{prefix}{msg.content}"})
                        multimodal_content.append({"type": "input_audio", "input_audio": {"data": b64, "format": "wav"}})
                except Exception as e:
                    logger.error(f"Voice B64 failed: {e}")
                    multimodal_content.append({"type": "text", "text": f"{prefix}[Voice Error]"})
            
            # Text / Processed-but-failed Media
            else:
                if msg.content:
                    text_content = msg.content
                    if msg.reply_to_content:
                        prefix += f'(Reply to "{msg.reply_to_content}") '
                    multimodal_content.append({"type": "text", "text": prefix + text_content})

        if multimodal_content:
            messages.append({"role": "user", "content": multimodal_content})
    else:
        # Should not happen if tail_msgs is empty, but just in case
        pass

    # 7. è°ƒç”¨ LLM
    current_temp = safe_float_config(configs.get("temperature", "0.7"), 0.7, 0.0, 2.0)
    
    try:
        client = AsyncOpenAI(api_key=api_key, base_url=base_url)
        # æ³¨æ„: modalities=["text"] åœ¨ audio preview æ¨¡å‹ä¸­é€šå¸¸æ˜¯å¿…é¡»çš„
        response = await client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=current_temp,
            max_tokens=4000,
            modalities=["text"] 
        )
        
        if not response.choices or not response.choices[0].message.content:
            await context.bot.send_message(chat_id, "âš ï¸ AI è¿”å›ç©ºå†…å®¹")
            return
            
        reply_content = response.choices[0].message.content.strip()
        logger.info(f"LLM Response: {reply_content[:100]}...")
        
        # 8. è§£æç»“æœå¹¶å›å¡« (Backfill)
        # æ­¤æ—¶ DB ä¸­çš„ content å·²ç»æ˜¯ [å›¾ç‰‡å†…å®¹: xxx] æˆ– [è¯­éŸ³è½¬å½•: xxx]
        # ä½†æˆ‘ä»¬å¯èƒ½æƒ³æŠŠ LLM ç”Ÿæˆçš„æ›´è¯¦ç»†çš„ Summary/Transcript è¦†ç›–è¿›å»ï¼Ÿ
        # æˆ–è€…ï¼ŒShift-Left äº§ç”Ÿçš„å°±æ˜¯ Truthï¼Œä¸éœ€è¦ LLM å†å›å¡«äº†ï¼Ÿ
        # æ—¢ç„¶æˆ‘ä»¬å·²ç»ç”¨ Shift-Left äº§ç”Ÿäº† Caption/Transcriptï¼ŒLLM çš„è¾“å‡ºåº”è¯¥ä¸»è¦é’ˆå¯¹ User Query
        # æ‰€ä»¥è¿™é‡Œçš„ å›å¡«é€»è¾‘ å¯ä»¥ç®€åŒ–/ç§»é™¤ï¼Œæˆ–è€…åªä½œä¸ºè¡¥å……ï¼Ÿ
        
        # ç°æœ‰çš„ Shift-Left å·²ç»åœ¨ RAG ä¹‹å‰æ›´æ–°äº†å†…å­˜ä¸­çš„ msg.content
        # å¹¶åœ¨ DB ä¸­ï¼ˆå¦‚æœéœ€è¦æŒä¹…åŒ–ï¼‰åº”è¯¥åœ¨ Shift-Left é˜¶æ®µå°± Update DB
        
        # æˆ‘ä»¬åœ¨ Shift-Left é˜¶æ®µåªæ˜¯ä¿®æ”¹äº† msg å¯¹è±¡å±æ€§ï¼Œå¹¶æ²¡æœ‰ history_service.update_message_content...
        # åº”è¯¥åœ¨ Shift-Left loop é‡ŒæŒä¹…åŒ–ã€‚
        
        # è¡¥æ•‘ï¼šåœ¨ Shift-Left å¾ªç¯é‡Œæ·»åŠ  await history_service.update_message_content_by_file_id(...)
        # å¦åˆ™ä¸‹ä¸€æ¬¡åŠ è½½åˆæ˜¯ Processing...
        
        # ç”±äº Shift-Left å·²ç»åœ¨ä¸Šé¢å¤„ç†äº†æŒä¹…åŒ– (Wait, I need to add that to the first chunk!)
        # Re-visiting the first chunk: I didn't add DB update there.
        
        # Let's add DB persistent updates here for processed items
        for mid, (mtype, content) in processed_media_cache.items():
            if mtype == 'image':
                 msg_obj, _ = pending_images_map.get(mid, (None,None))
                 if msg_obj:
                    # Persist Legacy Format: [Image Summary: caption]
                    await history_service.update_message_content_by_file_id(msg_obj.file_id, f"[Image Summary: {content}]")
                    
            elif mtype == 'voice':
                 # content is transcript
                 msg_obj, _ = pending_voices_map.get(mid, (None,None))
                 if msg_obj:
                     # Persist Legacy Format: Raw Text
                    await history_service.update_message_content_by_file_id(msg_obj.file_id, content)

        # 8.2 åŸæœ‰çš„ XML å›å¡«é€»è¾‘ (Optional Compatibility)
        # å¦‚æœ LLM ä¾ç„¶è¿”å›äº† <transcript> (å¯èƒ½å› ä¸º Prompt æ²¡æ”¹?)
        # æš‚æ—¶ä¿ç•™ä»¥é˜²ä¸‡ä¸€ï¼Œä½†ä¸»è¦ä¾èµ– Shift-Left
        
        # ... (Legacy XML parsing removed for cleanliness, ensuring Shift-Left is the source of truth)
            
        if not reply_content:
            reply_content = "<chat>...</chat>" # å…œåº•

        # 9. å‘é€å›å¤
        # åªè¦åŒ…å«è¯­éŸ³è¾“å…¥ï¼Œä¸€å¾‹é‡‡ç”¨è¯­éŸ³å“åº”
        reply_mtype = 'voice' if has_v else 'text'
        
        await sender_service.send_llm_reply(
            chat_id=chat_id,
            reply_content=reply_content,
            context=context,
            history_msgs=history_msgs,
            message_type=reply_mtype
        )


    except Exception as e:
        logger.error(f"API Call failed: {e}")
        # --- æ±¡æŸ“æ¸…ç†é€»è¾‘ ---
        # å¦‚æœå¤„ç†å¤±è´¥ï¼Œåˆ é™¤å½“å‰æ‰¹æ¬¡ä¸­å¤„äº "Processing..." çŠ¶æ€çš„å ä½æ¶ˆæ¯ï¼Œé˜²æ­¢æ±¡æŸ“ä¸Šä¸‹æ–‡
        # ä»…æ¸…é™¤é‚£äº›**å°šæœªå¤„ç†æˆåŠŸ**ï¼ˆä»æ˜¯ Processing å ä½ç¬¦ï¼‰çš„æ¶ˆæ¯ã€‚
        # å¦‚æœ Shift-Left å·²ç»æˆåŠŸç”Ÿæˆäº† Description/Transcript å¹¶æ›´æ–°äº† DBï¼Œåˆ™ä¿ç•™ã€‚
        try:
            async for session in get_db_session():
                # å¯»æ‰¾å½“å‰æ‰¹æ¬¡ä¸­æ‰€æœ‰ä»å¸¦ Processing æ ‡è¯†çš„æ¶ˆæ¯ ID
                # æ³¨æ„ï¼šShift-Left å¯èƒ½ä¼šä¿®æ”¹å†…å­˜ä¸­çš„ msg.contentï¼Œæ‰€ä»¥è¿™é‡Œåº”è¯¥å»æŸ¥è¯¢ DBï¼Œæˆ–è€…æ˜¯ä¾èµ– msg.content å¦‚æœ Shift-Left æ²¡è·‘æˆ–è€…æ˜¯å¤±è´¥äº†
                # ä½†æ˜¯ msg å¯¹è±¡æ˜¯å¼•ç”¨çš„ï¼Œæ‰€ä»¥åœ¨å†…å­˜ä¸­å¦‚æœæ˜¯ [å›¾ç‰‡å†…å®¹: ...] é‚£å°±ä¸åŒ¹é…äº† -> æ­£ç¡®ï¼Œå› ä¸ºé‚£æ˜¯æœ‰æ•ˆæ•°æ®ï¼
                # åªè¦åŒ¹é… [Image: Processing...] æˆ– [Voice: Processing...] å°±åˆ 
                
                pending_ids = [m.id for m in tail_msgs if "[Image: Processing...]" in m.content or "[Voice: Processing...]" in m.content]
                if pending_ids:
                    await session.execute(delete(History).where(History.id.in_(pending_ids)))
                    await session.commit()
                    logger.info(f"Context Cleanup: Removed {len(pending_ids)} pending placeholder(s) due to API failure.")
        except Exception as cleanup_err:
            logger.error(f"Failed to cleanup pending placeholders: {cleanup_err}")

        # å¼ºåˆ¶é€šçŸ¥ç®¡ç†å‘˜ (ç§èŠæ¨é€)
        try:
            error_msg = (
                f"ğŸš¨ <b>API Call Failed</b>\n\n"
                f"ä¼šè¯ ID: <code>{chat_id}</code>\n"
                f"é”™è¯¯è¯¦æƒ…: <code>{e}</code>\n\n"
                f"ğŸ’¡ <i>ä¸Šä¸‹æ–‡æ±¡æŸ“å·²è‡ªåŠ¨æ¸…ç†ï¼Œè¯·æ£€æŸ¥ API ä½™é¢æˆ–ç½‘ç»œç¯å¢ƒã€‚</i>"
            )
            await context.bot.send_message(settings.ADMIN_USER_ID, error_msg, parse_mode='HTML')
        except Exception as notify_err:
            logger.error(f"Failed to notify admin privately: {notify_err}")


async def process_reaction_update(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """å¤„ç†è¡¨æƒ…å›åº”æ›´æ–°"""
    reaction = update.message_reaction
    if not reaction:
        return
        
    chat = reaction.chat
    user = reaction.user
    message_id = reaction.message_id
    
    if chat.type == constants.ChatType.PRIVATE:
        return
    if not await access_service.is_whitelisted(chat.id):
        return
        
    if user and user.id == context.bot.id:
        return

    emojis = []
    for react in reaction.new_reaction:
        if hasattr(react, 'emoji'):
            emojis.append(react.emoji)
        elif hasattr(react, 'custom_emoji_id'):
            emojis.append('[CustomEmoji]')
            
    if not emojis:
        content = f"[System Info] {user.first_name if user else 'User'} removed reaction from [MSG {message_id}]"
    else:
        emoji_str = "".join(emojis)
        content = f"[System Info] {user.first_name if user else 'User'} reacted {emoji_str} to [MSG {message_id}]"

    logger.info(f"REACTION [{chat.id}]: {content}")
    
    await history_service.add_message(
        chat_id=chat.id,
        role="system",
        content=content
    )

lazy_sender.set_callback(generate_response)
